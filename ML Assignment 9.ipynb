{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d96707-3482-4e48-9674-758eb0479efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "# engineering in depth.\n",
    "\n",
    "# Ans: Feature engineering is the process of transforming raw data into meaningful features that can improve the performance of machine\n",
    "# learning models. It involves selecting, creating, and transforming variables to highlight important patterns and relationships in the \n",
    "# data. Key aspects of feature engineering include:\n",
    "\n",
    "# Feature Selection: Identifying the most relevant features that have a strong correlation with the target variable and discarding\n",
    "# irrelevant or redundant ones. This helps reduce dimensionality and avoids overfitting.\n",
    "\n",
    "# Feature Creation: Constructing new features by combining or transforming existing variables. This can involve mathematical operations,\n",
    "# domain knowledge, or creating interaction terms to capture complex relationships.\n",
    "\n",
    "# Handling Missing Data: Dealing with missing values in the dataset by imputing them using appropriate techniques such as mean, median,\n",
    "# or regression-based imputation.\n",
    "\n",
    "# Encoding Categorical Variables: Converting categorical variables into numerical representations that machine learning algorithms can \n",
    "# understand. This can be done using techniques like one-hot encoding, ordinal encoding, or target encoding.\n",
    "\n",
    "# Scaling and Normalization: Rescaling numerical features to a standard range (e.g., 0 to 1) to ensure that they have similar magnitudes.\n",
    "# This helps prevent certain features from dominating the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e195d0-a24d-4f43-be83-208de9e421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "# methods of function selection?\n",
    "\n",
    "# Ans: Feature selection is the process of choosing the most relevant features from a dataset to improve model performance and reduce\n",
    "# computational complexity. The aim is to select a subset of features that have the most predictive power while minimizing overfitting. \n",
    "# Various methods of feature selection include Filter methods (e.g., correlation, chi-square), Wrapper methods\n",
    "# (e.g., recursive feature elimination), and Embedded methods (e.g., Lasso regression, decision tree feature importance). \n",
    "# These methods assess the relevance and importance of features based on statistical measures, model performance, or embedded feature \n",
    "# selection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e442e103-04b6-4d43-9196-b9a7e003cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "\n",
    "#Ans:  Filter approaches for feature selection evaluate the relevance of features independently of the chosen machine learning algorithm. \n",
    "# They use statistical measures like correlation or chi-square to rank and select features. Pros: Fast computation, independence from \n",
    "# specific algorithms. Cons: Ignores feature interactions, may not consider the target variable.\n",
    "\n",
    "# Wrapper approaches evaluate feature subsets by training and evaluating the model iteratively. They use performance metrics \n",
    "# (e.g., accuracy) to select features. Pros: Considers feature interactions, specific to the chosen algorithm. Cons: Computationally \n",
    "# expensive, prone to overfitting, requires training multiple models.\n",
    "\n",
    "# Overall, filter approaches are computationally efficient but may not capture feature interactions, while wrapper approaches consider \n",
    "# feature interactions but can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a569946-fd9e-42c0-b283-1f0e936cbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. i. Describe the overall feature selection process.\n",
    "# ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction \n",
    "# algorithms?\n",
    "\n",
    "#Ans:  i. The overall feature selection process involves the following steps:\n",
    "\n",
    "# Data Understanding: Gain a thorough understanding of the data, including its structure and relationships.\n",
    "# Feature Generation: Create new features from existing ones using mathematical operations or domain knowledge.\n",
    "# Feature Selection: Evaluate the relevance of features using filter, wrapper, or embedded methods and select the most informative ones.\n",
    "# Model Training: Train a machine learning model using the selected features.\n",
    "# Model Evaluation: Assess the model's performance using appropriate evaluation metrics.\n",
    "# Iterative Refinement: Iterate through steps 2-5 to improve the feature selection and model performance.\n",
    "\n",
    "# ii. The key principle of feature extraction is to transform the raw data into a lower-dimensional representation that retains the\n",
    "# most relevant information. An example is Principal Component Analysis (PCA), which identifies the directions of maximum variance in\n",
    "# the data and projects it onto a lower-dimensional space. This technique is used to reduce the dimensionality of high-dimensional data \n",
    "# while preserving its essential characteristics. Other widely used feature extraction algorithms include Linear Discriminant Analysis\n",
    "# (LDA) for supervised dimensionality reduction and Non-negative Matrix Factorization (NMF) for non-linear feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc1e32d-b4a8-4069-839f-db04fba6c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "# Ans: In a text categorization issue, the feature engineering process involves transforming textual data into numerical features that can be\n",
    "# used by machine learning algorithms. The process includes steps such as:\n",
    "\n",
    "# Text Preprocessing: Cleaning the text by removing punctuation, stop words, and performing stemming or lemmatization.\n",
    "# Tokenization: Breaking the text into individual words or tokens.\n",
    "# Feature Creation: Generating numerical features from the text, such as bag-of-words representation, TF-IDF values, or word embeddings.\n",
    "# Feature Selection: Selecting relevant features based on their frequency, importance, or correlation with the target categories.\n",
    "# Encoding: Converting categorical variables like labels or sentiment into numerical representations.\n",
    "# Model Training: Training a machine learning model on the selected features.\n",
    "# Evaluation: Assessing the model's performance using appropriate metrics and refining the feature engineering process if necessary.\n",
    "# The goal is to transform the text data into meaningful and informative features that capture the essence of the text content and enable\n",
    "# accurate categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfcfcaa5-b8a9-4ed6-8df4-860d424ee732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of \n",
    "# (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "#Ans:  Cosine similarity is a good metric for text categorization because it measures the similarity between two documents based on the \n",
    "# angle between their feature vectors in a high-dimensional space. It is particularly suitable for text data because it focuses on the \n",
    "# direction of the vectors rather than their magnitudes. Cosine similarity ranges from -1 to 1, with 1 indicating identical documents\n",
    "# and 0 indicating no similarity.\n",
    "\n",
    "# Given the document-term matrix with rows (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), we can calculate the cosine similarity as follows:\n",
    "\n",
    "# Calculate the dot product of the two vectors: (22 + 31 + 20 + 00 + 23 + 32 + 31 + 03 + 1*1) = 24.\n",
    "# Calculate the magnitude of each vector: sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) ≈ 6.48 for the first vector\n",
    "# and sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) ≈ 5.29 for the second vector.\n",
    "# Divide the dot product by the product of the vector magnitudes: 24 / (6.48 * 5.29) ≈ 0.72.\n",
    "# Therefore, the resemblance in cosine between the two rows is approximately 0.72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6fedc8-8c9d-4887-b2f0-5706437affbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "# ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,0, 1, 0, 1, 1) \n",
    "# and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "# Ans:  Cosine similarity is a good metric for text categorization because it measures the similarity between two documents based on the\n",
    "# angle between their feature vectors in a high-dimensional space. It is particularly suitable for text data because it focuses on the \n",
    "# direction of the vectors rather than their magnitudes. Cosine similarity ranges from -1 to 1, with 1 indicating identical documents \n",
    "# and 0 indicating no similarity.\n",
    "\n",
    "# Given the document-term matrix with rows (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), we can calculate the cosine \n",
    "# similarity as follows:\n",
    "\n",
    "# Calculate the dot product of the two vectors: (22 + 31 + 20 + 00 + 23 + 32 + 31 + 03 + 1*1) = 24.\n",
    "# Calculate the magnitude of each vector: sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) ≈ 6.48 for the first vector and \n",
    "# sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) ≈ 5.29 for the second vector.\n",
    "# Divide the dot product by the product of the vector magnitudes: 24 / (6.48 * 5.29) ≈ 0.72.\n",
    "# Therefore, the resemblance in cosine between the two rows is approximately 0.72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e6b028-941f-4a97-8958-df20c0e8d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "# What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "# What can be done about it?\n",
    "\n",
    "# Ans: A high-dimensional dataset refers to a dataset with a large number of features or variables relative to the number of observations.\n",
    "# In other words, the dataset contains a substantial amount of dimensions compared to the available data points. Real-life examples of \n",
    "# high-dimensional datasets include genomics data with thousands of genes, images with numerous pixels, or text documents with a large \n",
    "# vocabulary.\n",
    "\n",
    "# Difficulties in using machine learning techniques on high-dimensional datasets include:\n",
    "\n",
    "# Curse of Dimensionality: As the number of dimensions increases, the data becomes increasingly sparse, making it challenging to \n",
    "# find meaningful patterns and relationships.\n",
    "\n",
    "# Increased Computational Complexity: Many machine learning algorithms struggle with high-dimensional data due to the increased \n",
    "# computational requirements and the need for more training samples.\n",
    "\n",
    "# To address these challenges, several techniques can be employed:\n",
    "\n",
    "# Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) or t-SNE can reduce the dimensionality while preserving\n",
    "# important patterns and variability in the data.\n",
    "\n",
    "# Feature Selection: Choosing the most relevant features from the high-dimensional dataset can help reduce noise and improve model\n",
    "# performance. Techniques like filter methods, wrapper methods, or embedded methods can be used for feature selection.\n",
    "\n",
    "# Regularization: Applying regularization techniques such as L1 or L2 regularization can help prevent overfitting and provide a \n",
    "# balance between model complexity and performance.\n",
    "\n",
    "# Ensemble Methods: Utilizing ensemble methods like random forests or gradient boosting can handle high-dimensional data by combining \n",
    "# multiple models to make accurate predictions.\n",
    "\n",
    "# By employing these techniques, the challenges associated with high-dimensional datasets can be mitigated, allowing for more effective \n",
    "# application of machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a958204-11f5-4691-96cb-e47ef17a9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Make a few quick notes on:\n",
    "# PCA is an acronym for Personal Computer Analysis.\n",
    "# 2. Use of vectors\n",
    "# 3. Embedded technique\n",
    "\n",
    "# Ans: \n",
    "# PCA: PCA stands for Principal Component Analysis, not Personal Computer Analysis. It is a dimensionality reduction technique used to\n",
    "# transform high-dimensional data into a lower-dimensional space while retaining the most important patterns and variability in the data.\n",
    "# PCA identifies the principal components, which are linear combinations of the original features, capturing the maximum variance in\n",
    "# the data.\n",
    "\n",
    "# Use of Vectors: Vectors are commonly used in machine learning and data analysis to represent data points or features.\n",
    "# In high-dimensional datasets, each data point is often represented as a vector with each dimension corresponding to a specific feature.\n",
    "# Vectors are used to perform mathematical operations, measure distances, calculate similarities, and perform transformations on the data.\n",
    "\n",
    "# Embedded Technique: An embedded technique in feature selection refers to methods that incorporate the feature selection process within \n",
    "# the model training process. These techniques learn feature importance or relevance as part of the model training process itself. \n",
    "# Examples include Lasso regression, decision tree feature importance, or ridge regression, where feature selection is inherently embedded \n",
    "# within the algorithm. This approach simplifies the feature selection process by integrating it directly into the model training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cf24cb0-3e26-4c4a-9706-5ac8c7401d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Make a comparison between:\n",
    "# 1. Sequential backward exclusion vs. sequential forward selection\n",
    "# 2. Function selection methods: filter vs. wrapper\n",
    "# 3. SMC vs. Jaccard coefficient\n",
    "\n",
    "# Ans: \n",
    "# Sequential backward exclusion vs. sequential forward selection: Backward exclusion removes features iteratively,\n",
    "# while forward selection adds features incrementally.\n",
    "\n",
    "# Filter vs. wrapper function selection methods: Filter methods evaluate features independently of the model, while wrapper\n",
    "# methods use a specific model to evaluate feature subsets.\n",
    "\n",
    "# SMC vs. Jaccard coefficient: SMC measures binary variable similarity, while Jaccard coefficient measures set similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a356b-d880-4fea-bfc2-7c6ce211bcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
