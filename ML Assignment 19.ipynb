{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63bcc47f-4b57-4bb4-a06f-a50f8b85b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "# and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "# a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "# b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "# Ans:\n",
    "# a) Using the k-means method, we can create two clusters for each set of centroids:\n",
    "\n",
    "# Set 1 Centroids: 15, 32\n",
    "# Initial Cluster Assignment:\n",
    "\n",
    "# Data points 5, 10, 15 are closer to centroid 15.\n",
    "# Data points 20, 25, 30, 35 are closer to centroid 32.\n",
    "# Updated Cluster Assignment:\n",
    "# Cluster 1: 5, 10, 15\n",
    "# Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "# Set 2 Centroids: 12, 30\n",
    "# Initial Cluster Assignment:\n",
    "\n",
    "# Data points 5, 10, 15 are closer to centroid 12.\n",
    "# Data points 20, 25, 30, 35 are closer to centroid 30.\n",
    "# Updated Cluster Assignment:\n",
    "# Cluster 1: 5, 10, 15\n",
    "# Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "# b) To calculate the Sum of Squared Errors (SSE) for each set of centroid values, we need to compute the squared distance between \n",
    "# each data point and its assigned centroid, and then sum these squared distances for all data points in each cluster.\n",
    "\n",
    "# Set 1 SSE calculation:\n",
    "# SSE1 = (5-15)^2 + (10-15)^2 + (15-15)^2 + (20-32)^2 + (25-32)^2 + (30-32)^2 + (35-32)^2\n",
    "# = 100 + 25 + 0 + 144 + 49 + 4 + 9\n",
    "# = 331\n",
    "\n",
    "# Set 2 SSE calculation:\n",
    "# SSE2 = (5-12)^2 + (10-12)^2 + (15-12)^2 + (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2\n",
    "# = 49 + 4 + 9 + 100 + 25 + 0 + 25\n",
    "# = 212\n",
    "\n",
    "# Therefore, the SSE for Set 1 is 331 and the SSE for Set 2 is 212."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed9965b-9cb0-4807-8b4f-0429b2260461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "# Ans:\n",
    "# Market Basket Analysis uses association analysis concepts to uncover patterns and relationships in customer purchasing behavior.\n",
    "# It identifies frequent itemsets and generates association rules based on the co-occurrence of items in transactions. These rules reveal\n",
    "# the associations between products and help businesses understand customer preferences, optimize product placement, and develop \n",
    "# targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba825e7-7e47-4208-b40e-86553eda9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "# Ans:\n",
    "# Apriori algorithm example:\n",
    "\n",
    "# Given a transaction dataset:\n",
    "\n",
    "# TID Items\n",
    "# 1 Bread, Milk\n",
    "# 2 Bread, Diapers, Beer\n",
    "# 3 Milk, Diapers, Eggs\n",
    "# 4 Bread, Milk, Diapers, Beer\n",
    "# 5 Bread, Milk, Diapers, Eggs\n",
    "\n",
    "# Find frequent 1-itemsets:\n",
    "# {Bread}, {Milk}, {Diapers}\n",
    "\n",
    "# Generate candidate 2-itemsets:\n",
    "# {Bread, Milk}, {Bread, Diapers}, {Milk, Diapers}\n",
    "\n",
    "# Find frequent 2-itemsets:\n",
    "# {Bread, Milk}, {Bread, Diapers}, {Milk, Diapers}\n",
    "\n",
    "# Generate candidate 3-itemsets:\n",
    "# {Bread, Milk, Diapers}\n",
    "\n",
    "# Find frequent 3-itemsets:\n",
    "# {Bread, Milk, Diapers}\n",
    "\n",
    "# Generate association rules:\n",
    "# {Bread, Milk} => {Diapers}\n",
    "\n",
    "# These association rules indicate that if a customer buys bread and milk, they are likely to buy diapers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a993b63-1bd1-490f-8cee-31da41354c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "# is used to decide when to end the iteration.\n",
    "\n",
    "# Ans:\n",
    "# In hierarchical clustering, the distance between clusters is typically measured using a distance metric such as Euclidean distance,\n",
    "# Manhattan distance, or correlation distance. The choice of distance metric depends on the nature of the data and the specific \n",
    "# requirements of the clustering task.\n",
    "\n",
    "# The iteration in hierarchical clustering continues until all data points are assigned to a single cluster or until a stopping criterion\n",
    "# is met. One common stopping criterion is based on a threshold distance or a predetermined number of desired clusters. \n",
    "# The distance metric is used to decide when to end the iteration by comparing the distances between clusters at each step.\n",
    "\n",
    "# At each iteration, the algorithm merges the two closest clusters based on the chosen distance metric. The distance between clusters\n",
    "# can be computed using different methods such as single linkage, complete linkage, or average linkage. The specific linkage method\n",
    "# determines how the distance between clusters is calculated.\n",
    "\n",
    "# The iteration ends when the distance between the remaining clusters exceeds the defined threshold or when the desired number of \n",
    "# clusters is reached. This process forms a hierarchical structure or a dendrogram, where the vertical axis represents the distance\n",
    "# between clusters. By setting a threshold or desired number of clusters, the algorithm determines the appropriate stopping point\n",
    "# and partitions the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a2099c-9860-4de9-b35a-fc3c88ab03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "# Ans:\n",
    "# In the k-means algorithm, the recomputation of cluster centroids occurs in the following steps:\n",
    "\n",
    "# Initialization: Initially, k centroids are randomly chosen as the center points of the clusters.\n",
    "\n",
    "# Assignment: Each data point is assigned to the nearest centroid based on a chosen distance metric (usually Euclidean distance).\n",
    "\n",
    "# Update: After the assignment step, the cluster memberships are fixed. The centroids are updated by computing the mean of the data\n",
    "# points within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43bf27c4-aa5a-40fe-a804-fd6da7a03497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "# clusters.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The elbow method is a popular technique for determining the required number of clusters in a clustering exercise.\n",
    "# It involves calculating the within-cluster sum of squares (WCSS) for different values of k and selecting the point on the plot\n",
    "# where the decrease in WCSS levels off significantly as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648032a3-d45b-4455-ae19-dc12be4a826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Advantages of the k-means algorithm:\n",
    "\n",
    "# Simple and easy to understand.\n",
    "# Computationally efficient and can handle large datasets.\n",
    "# Works well with spherical and well-separated clusters.\n",
    "# Converges to a local optimum, which can be sufficient for many practical applications.\n",
    "# Disadvantages of the k-means algorithm:\n",
    "\n",
    "# Requires the number of clusters (k) to be specified in advance.\n",
    "# Sensitive to initial centroid selection, which can lead to different results.\n",
    "# Assumes clusters of similar size and density.\n",
    "# Not suitable for non-linear or irregularly shaped clusters.\n",
    "# Can be affected by outliers, as they can disproportionately influence cluster centroids.\n",
    "# Overall, while the k-means algorithm has its limitations, it remains a popular and widely used clustering algorithm due to its \n",
    "# simplicity and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e9a1ba-7f2a-47c2-9119-c0897cd1da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "# Ans:\n",
    "# A clustering diagram consists of scattered data points in a two-dimensional space, with each point representing a data sample. \n",
    "# Clusters are formed by grouping together data points that are close to each other, and each cluster is represented by a distinct \n",
    "# shape or boundary. The goal is to identify natural groupings or patterns in the data based on proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeff1029-09a8-4a2f-a90c-d2258dcdbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "# the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "# C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "# C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "# C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "# C3: (5,5) and (9,9)\n",
    "\n",
    "# Ans:\n",
    "# In the given scenario, the team is using the k-means algorithm to cluster 20 defect data points into 5 clusters. \n",
    "# The k-means algorithm starts by randomly initializing 5 cluster centroids. It then iteratively assigns each data point to\n",
    "# the nearest centroid and updates the centroids based on the assigned points. This process continues until convergence, where the\n",
    "# centroids remain unchanged.\n",
    "\n",
    "# Once the clustering process is complete, each defect data point will be assigned to one of the 5 identified clusters.\n",
    "# Any new defect that arises after clustering must also be assigned to one of these clusters based on its similarity to the \n",
    "# existing clusters. This ensures that all defects are categorized into one of the predefined forms identified by clustering.\n",
    "\n",
    "# The diagram would visually represent the 20 defect data points scattered in a space and the resulting clusters formed by the k-means \n",
    "# algorithm. Each data point would be connected to the centroid of its assigned cluster. However, as a text-based interface,\n",
    "# I'm unable to provide a diagram here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d551aa-6d4e-49a0-b869-cbab7493e172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignments:\n",
      "[3 3 3 1 1 1 1 4 4 4 4 0 0 0 0 0 2 2 2 2]\n",
      "\n",
      "Centroids:\n",
      "[[13.  13. ]\n",
      " [ 4.5  4.5]\n",
      " [17.5 17.5]\n",
      " [ 1.   1. ]\n",
      " [ 8.5  8.5]]\n",
      "\n",
      "New Defect Cluster:\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# 10. In a software project, the team is attempting to determine if software flaws discovered during\n",
    "# testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "# of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "# be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "# process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "# k-means algorithm.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Defect data points\n",
    "defects = np.array([[x, x] for x in range(20)])\n",
    "\n",
    "# K-means clustering with 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "kmeans.fit(defects)\n",
    "\n",
    "# Cluster assignments for each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Centroids of the clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# New defect\n",
    "new_defect = np.array([[25, 25]])\n",
    "\n",
    "# Predicting the cluster for the new defect\n",
    "new_defect_cluster = kmeans.predict(new_defect)\n",
    "\n",
    "# Print cluster assignments and centroids\n",
    "print(\"Cluster Assignments:\")\n",
    "print(cluster_labels)\n",
    "print(\"\\nCentroids:\")\n",
    "print(centroids)\n",
    "print(\"\\nNew Defect Cluster:\")\n",
    "print(new_defect_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77f647-e010-42c1-9871-24e467d6d63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
