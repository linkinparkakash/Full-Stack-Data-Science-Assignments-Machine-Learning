{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b2da96-5b49-43a3-a8ad-c1af69eb34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?\n",
    "\n",
    "# Ans:\n",
    "# The estimated depth of a Decision Tree trained on a one million instance training set can vary depending on the complexity of \n",
    "# the data and the specific algorithm used. However, it is generally expected that the depth of the tree will be relatively \n",
    "# deep, potentially reaching several levels, to capture the intricacies of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc727544-5d77-4e19-ac1a-c467842da97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually\n",
    "# lower/greater?\n",
    "\n",
    "# Ans:\n",
    "# The Gini impurity of a node is typically lower than or equal to that of its parent node. It is not always lower or always greater, \n",
    "# as it depends on the specific splitting criterion and the distribution of classes within the node. However, in most cases, the Gini \n",
    "# impurity tends to decrease as we move from the parent node to its child nodes, as the goal of splitting is to create subsets with \n",
    "# more homogeneous class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed4ae92-f1f4-47a8-94c4-d07163c26eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "\n",
    "# Ans:\n",
    "# Reducing the maximum depth of a Decision Tree can be a good idea if the model is overfitting the training set. \n",
    "# Overfitting occurs when the tree becomes too complex, capturing noise and outliers in the data instead of general patterns. \n",
    "# By reducing the maximum depth, we limit the tree's ability to create complex and detailed splits, which helps prevent overfitting.\n",
    "\n",
    "# A shallower tree with a reduced maximum depth tends to have higher bias and lower variance. It may sacrifice some of the model's\n",
    "# ability to capture intricate patterns in the training data but can improve its ability to generalize to unseen data.\n",
    "# It can result in a simpler and more interpretable model that avoids overfitting and performs better on test data or new observations.\n",
    "\n",
    "# However, the optimal maximum depth depends on the specific dataset and problem. It is important to find the right balance between \n",
    "# underfitting and overfitting by tuning the hyperparameters and evaluating the model's performance on a validation or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5653e463-3c01-43ad-81f7-327f69410be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set?\n",
    "\n",
    "# Ans:\n",
    "# Scaling the input features is generally not necessary or beneficial for a Decision Tree if it is underfitting the training set. \n",
    "# Decision Trees are not sensitive to the scale of the input features because they make decisions based on feature thresholds rather \n",
    "# than the actual feature values.\n",
    "\n",
    "# Underfitting occurs when the Decision Tree is too simple and fails to capture the underlying patterns in the data. \n",
    "# Scaling the input features does not address this issue because it does not affect the structure or complexity of the tree.\n",
    "\n",
    "# Instead of scaling the features, other approaches should be considered to address underfitting in a Decision Tree. \n",
    "# Some possible solutions include increasing the maximum depth of the tree, allowing more splits and complexity, adding \n",
    "# more relevant features, or using ensemble techniques such as Random Forests to combine multiple trees.\n",
    "\n",
    "# However, it's important to note that if there are other algorithms or models in the pipeline that rely on scaled features, \n",
    "# then scaling may be necessary for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb291c0a-5847-4f20-9320-01a3f85280cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How much time will it take to train another Decision Tree on a training set of 10 million instances\n",
    "#  it takes an hour to train a Decision Tree on a training set with 1 million instances?\n",
    "\n",
    "# Ans:\n",
    "# Assuming that the time to train a Decision Tree is directly proportional to the size of the training set, \n",
    "# we can estimate the time it will take to train a Decision Tree on a training set of 10 million instances.\n",
    "\n",
    "# If it takes 1 hour to train a Decision Tree on a training set of 1 million instances, we can use the concept of proportionality\n",
    "# to estimate the time for a 10 million-instance training set.\n",
    "\n",
    "# The ratio between the training set sizes is 10 million / 1 million = 10. So, we can assume that training a Decision Tree on a 10 \n",
    "# million-instance training set will take approximately 10 times longer than training on a 1 million-instance training set.\n",
    "\n",
    "# Therefore, it can be estimated that training another Decision Tree on a training set of 10 million instances would take approximately \n",
    "# 10 hours. However, it's important to note that this is a rough estimate, and the actual training time can be influenced by various\n",
    "# factors such as hardware capabilities, optimization techniques, and complexity of the data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272fc249-bb2b-4777-848c-8aaf5a1c5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n",
    "# Ans:\n",
    "# Setting presort=True in a Decision Tree algorithm specifies that the training data should be presorted to improve the speed of training.\n",
    "# However, whether it will actually speed up training depends on the specific characteristics of the dataset and the implementation of \n",
    "# the algorithm.\n",
    "\n",
    "# For smaller datasets, such as the one with 100,000 instances mentioned in the question, the overhead of presorting the data may outweigh \n",
    "# the potential speed improvements. Presorting requires additional computational resources and time upfront to sort the data,\n",
    "# which may not be efficient for smaller datasets.\n",
    "\n",
    "# In general, the decision to use presort=True should be based on the size and complexity of the dataset, the available computational \n",
    "# resources, and the specific implementation of the algorithm. It is recommended to try training with and without presorting and compare \n",
    "# the training times to determine the effectiveness of using presort=True for a particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054a4b14-04ff-4130-832f-b57d4c0b1e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "# 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "# a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "# b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "# c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "# validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "# d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "# output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step a: Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "\n",
    "# Step b: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step c: Perform grid search to find the best hyperparameters\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Step d: Train the model with the best hyperparameters on the entire training set\n",
    "clf = DecisionTreeClassifier(**best_params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f8e784-f112-4782-8366-a3a51adebeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.871\n",
      "Accuracy Improvement: 0.039000000000000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4781/381836745.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  ensemble_predictions = mode(predictions, axis=0)[0]\n"
     ]
    }
   ],
   "source": [
    "# 8. Follow these steps to grow a forest:\n",
    "\n",
    "# a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "# 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "\n",
    "# b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "# Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "\n",
    "# Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "# since they were trained on smaller sets.\n",
    "\n",
    "# c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "#  keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test\n",
    "# collection, this method gives you majority-vote predictions.\n",
    "\n",
    "# d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "# than the first model (approx 0.5 to 1.5 percent higher).\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Step a: Create subsets of the training set\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "rs = ShuffleSplit(n_splits=1000, train_size=100, random_state=42)\n",
    "subsets = []\n",
    "\n",
    "for train_index, _ in rs.split(X):\n",
    "    X_subset, y_subset = X[train_index], y[train_index]\n",
    "    subsets.append((X_subset, y_subset))\n",
    "\n",
    "# Step b: Train Decision Trees on each subset and evaluate on the test set\n",
    "dtrees = []\n",
    "accuracies = []\n",
    "\n",
    "for X_subset, y_subset in subsets:\n",
    "    dtree = DecisionTreeClassifier(**best_params)\n",
    "    dtree.fit(X_subset, y_subset)\n",
    "    dtrees.append(dtree)\n",
    "    accuracy = dtree.score(X_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Step c: Make predictions using the ensemble of Decision Trees\n",
    "predictions = np.array([dtree.predict(X_test) for dtree in dtrees])\n",
    "ensemble_predictions = mode(predictions, axis=0)[0]\n",
    "\n",
    "# Step d: Evaluate the ensemble predictions on the test set\n",
    "ensemble_accuracy = np.mean(ensemble_predictions == y_test)\n",
    "improvement = ensemble_accuracy - accuracy\n",
    "\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Accuracy Improvement:\", improvement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca6110-23f5-4d10-8832-94665ca0dc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
