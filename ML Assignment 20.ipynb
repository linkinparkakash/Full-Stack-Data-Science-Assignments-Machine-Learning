{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf6ef64-e0c7-457d-9819-2b4696c7e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "# Ans:\n",
    "# The underlying concept of Support Vector Machines (SVM) is to find an optimal hyperplane that separates data points of different\n",
    "# classes by maximizing the margin or distance between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6184e419-4084-4064-8a4d-08364365bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the concept of a support vector?\n",
    "\n",
    "# Ans:\n",
    "# In Support Vector Machines (SVM), support vectors are the data points that lie closest to the decision boundary or hyperplane. \n",
    "# They play a crucial role in defining the decision boundary and determining the optimal separation between classes. \n",
    "# These support vectors support or influence the placement and orientation of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1f8fa2-7694-45a6-91fc-841cdacefb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "# Ans:\n",
    "# Scaling the inputs is necessary when using Support Vector Machines (SVM) for several reasons:\n",
    "\n",
    "# SVMs are sensitive to the scale of the input features. Features with larger scales can dominate the optimization process and have a \n",
    "# disproportionate impact on the decision boundary.\n",
    "\n",
    "# Scaling ensures that all features contribute equally to the SVM model. By bringing all features to a similar scale, the SVM can give\n",
    "# equal weight and importance to each feature during the training process.\n",
    "\n",
    "# Scaling helps improve the convergence of the optimization algorithm. It can lead to faster convergence and more stable results.\n",
    "\n",
    "# Overall, scaling the inputs in SVMs helps prevent biased or skewed results and ensures that the model can effectively learn from all\n",
    "# features in a fair and balanced manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b70170-d8b4-48f8-bcfc-650b736c4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "# Ans:\n",
    "# Yes, an SVM classifier can provide a confidence score or a percentage chance associated with its classification decision,\n",
    "# depending on the specific implementation and configuration.\n",
    "\n",
    "# In some SVM implementations, the distance from a data point to the decision boundary can be used as a confidence score. A larger\n",
    "# distance indicates higher confidence in the classification, as the data point is further away from the decision boundary.\n",
    "\n",
    "# Additionally, some SVM variants, such as the probabilistic SVM, provide a probability estimate or percentage chance for each class label. These models use techniques like Platt scaling or sigmoid calibration to map the SVM's output scores or distances to probabilities.\n",
    "\n",
    "# It's important to note that not all SVM implementations directly output probabilities or confidence scores by default. Some may \n",
    "# require additional calibration or post-processing techniques to obtain probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fc3cb0-c5c9-47a2-8549-a9f04d5e6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual \n",
    "# form of the SVM problem?\n",
    "\n",
    "# Ans:\n",
    "# When training a model on a training set with millions of instances and hundreds of features, it is generally recommended to use \n",
    "# the dual form of the SVM problem.\n",
    "\n",
    "# The dual form of SVM is more computationally efficient in scenarios where the number of features is large compared to the number \n",
    "# of instances. It avoids the need to explicitly calculate the feature vectors in the high-dimensional space, which can be computationally \n",
    "# expensive.\n",
    "\n",
    "# By using the dual form, the SVM training process can be optimized to efficiently handle high-dimensional datasets. It leverages kernel\n",
    "# functions to implicitly map the input data into a higher-dimensional space without explicitly calculating the transformed feature vectors.\n",
    "\n",
    "# However, it is important to note that the choice between the primal and dual form of SVM can depend on various factors, such as the \n",
    "# specific implementation, the available computational resources, and the characteristics of the dataset. It is advisable to consider \n",
    "# these factors and conduct empirical evaluations to determine the most suitable approach for a given scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab94887e-6774-4736-a4bb-535cd774aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "# training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "# Ans:\n",
    "# If an SVM classifier trained with an RBF (Radial Basis Function) kernel is underfitting the training data, you can consider adjusting\n",
    "# the hyperparameters gamma and C.\n",
    "\n",
    "# Gamma (Î³): Gamma determines the influence of a single training example on the decision boundary. To address underfitting, you can try\n",
    "# increasing gamma. Higher gamma values make the SVM model more sensitive to individual data points, potentially leading to a more\n",
    "# complex and flexible decision boundary.\n",
    "\n",
    "# C: C is the regularization parameter that controls the trade-off between achieving a larger margin and minimizing training errors. \n",
    "# To address underfitting, you can try decreasing C. Lower values of C allow for a wider margin and a more relaxed decision boundary,\n",
    "# which can better accommodate misclassified or overlapping training instances.\n",
    "\n",
    "# In summary, for an SVM classifier with an RBF kernel that underfits the training data, you can try raising gamma to make the model \n",
    "# more sensitive to individual data points and/or lower C to allow for a wider margin and more flexibility in the decision boundary.\n",
    "# However, the exact values of gamma and C should be chosen through experimentation and validation on a held-out dataset to find the \n",
    "# optimal balance and improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509bdef9-b564-4082-8e6b-6fcd96c01d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "# the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "# Ans:\n",
    "# To solve the soft margin linear SVM classifier problem using an off-the-shelf Quadratic Programming (QP) solver,\n",
    "# the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "# H: The Hessian matrix (or its approximation) represents the quadratic term of the objective function. In the case of a soft margin SVM,\n",
    "# H is typically set as the identity matrix or a diagonal matrix with regularization coefficients.\n",
    "\n",
    "# f: The f vector represents the linear term of the objective function. It depends on the data and labels, and it is used to minimize \n",
    "# the classification errors. The values in the f vector depend on the regularization parameters and the training data.\n",
    "\n",
    "# A: The A matrix represents the constraints on the decision boundary. For a soft margin SVM, it includes both the inequality constraints\n",
    "# for the margin and the equality constraints for the misclassified instances. The exact values and structure of the A matrix depend on\n",
    "# the training data, labels, and regularization parameters.\n",
    "\n",
    "# b: The b vector represents the right-hand side of the constraints. It contains the margin and misclassification constraints' thresholds.\n",
    "# The values in the b vector depend on the training data, labels, and regularization parameters.\n",
    "\n",
    "# It's worth noting that the exact formulation of the QP parameters may vary slightly depending on the specific QP solver being used. \n",
    "# It is recommended to consult the documentation or guidelines provided by the QP solver to ensure the correct parameter setting for \n",
    "# solving the soft margin linear SVM classifier problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5abff1b8-3b83-4b9a-a962-a802789fcfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "# an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "# Ans:\n",
    "# LinearSVC:\n",
    "\n",
    "# Import LinearSVC from the scikit-learn library.\n",
    "# Create an instance of LinearSVC and fit it to your linearly separable dataset.\n",
    "# Retrieve the learned model parameters, such as coefficients and intercept.\n",
    "# SVC:\n",
    "\n",
    "# Import SVC from the scikit-learn library.\n",
    "# Create an instance of SVC with a linear kernel (e.g., kernel='linear') and fit it to your linearly separable dataset.\n",
    "# Retrieve the learned model parameters, such as coefficients and intercept.\n",
    "# SGDClassifier:\n",
    "\n",
    "# Import SGDClassifier from the scikit-learn library.\n",
    "# Create an instance of SGDClassifier with a loss function suitable for linear classification (e.g., loss='hinge') and fit it to your\n",
    "# linearly separable dataset.\n",
    "# Retrieve the learned model parameters, such as coefficients and intercept.\n",
    "# After training the models, you can compare the learned parameters (coefficients and intercepts) across the three models.\n",
    "# If the dataset is indeed linearly separable, you would expect the models to have similar coefficients and intercepts.\n",
    "\n",
    "# It's important to note that due to the inherent randomness in the training process, the exact values of the model parameters may \n",
    "# differ slightly between the models. However, the overall pattern and separation should be similar.\n",
    "\n",
    "# By comparing the models, you can assess their performance and see if they provide similar decision boundaries for the linearly\n",
    "# separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f844591-abcf-4050-939b-bf0794f2d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM \n",
    "# classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "# to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "# Ans:\n",
    "# When training an SVM classifier on the MNIST dataset, using the one-versus-the-rest (OvR) strategy to classify all 10 digits,\n",
    "# it is possible to achieve a high level of precision. By tuning the hyperparameters and utilizing small validation sets for faster \n",
    "# experimentation, you can optimize the performance of the SVM classifier.\n",
    "\n",
    "# The level of precision that can be achieved depends on various factors, including the choice of SVM variant (linear, kernel-based), \n",
    "# hyperparameter settings (such as regularization parameter C and kernel parameters), feature engineering techniques, and \n",
    "# the size and quality of the training dataset.\n",
    "\n",
    "# With proper hyperparameter tuning and feature engineering, it is possible to achieve precision scores above 95% or even higher on \n",
    "# the MNIST dataset. However, the exact level of precision achieved will vary based on the specific implementation and the strategies \n",
    "# employed during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b105df-ee33-47da-8798-880a1488e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3570026426754463\n"
     ]
    }
   ],
   "source": [
    "# 10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "# Solution:\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM regressor\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# Train the SVM regressor\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1bc03-b795-4744-b100-5d1f3f222889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
