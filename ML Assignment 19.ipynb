{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a98c7660-2b48-498a-9d18-5f755519e043",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "Ans:\n",
    "a) Using the k-means method, we can create two clusters for each set of centroids:\n",
    "\n",
    "Set 1 Centroids: 15, 32\n",
    "Initial Cluster Assignment:\n",
    "\n",
    "Data points 5, 10, 15 are closer to centroid 15.\n",
    "Data points 20, 25, 30, 35 are closer to centroid 32.\n",
    "Updated Cluster Assignment:\n",
    "Cluster 1: 5, 10, 15\n",
    "Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "Set 2 Centroids: 12, 30\n",
    "Initial Cluster Assignment:\n",
    "\n",
    "Data points 5, 10, 15 are closer to centroid 12.\n",
    "Data points 20, 25, 30, 35 are closer to centroid 30.\n",
    "Updated Cluster Assignment:\n",
    "Cluster 1: 5, 10, 15\n",
    "Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "b) To calculate the Sum of Squared Errors (SSE) for each set of centroid values, we need to compute the squared distance between each data point and its assigned centroid, and then sum these squared distances for all data points in each cluster.\n",
    "\n",
    "Set 1 SSE calculation:\n",
    "SSE1 = (5-15)^2 + (10-15)^2 + (15-15)^2 + (20-32)^2 + (25-32)^2 + (30-32)^2 + (35-32)^2\n",
    "= 100 + 25 + 0 + 144 + 49 + 4 + 9\n",
    "= 331\n",
    "\n",
    "Set 2 SSE calculation:\n",
    "SSE2 = (5-12)^2 + (10-12)^2 + (15-12)^2 + (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2\n",
    "= 49 + 4 + 9 + 100 + 25 + 0 + 25\n",
    "= 212\n",
    "\n",
    "Therefore, the SSE for Set 1 is 331 and the SSE for Set 2 is 212."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d5a0064-dcff-49fd-a004-968aa683cb5a",
   "metadata": {},
   "source": [
    "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Market Basket Analysis uses association analysis concepts to uncover patterns and relationships in customer purchasing behavior. It identifies frequent itemsets and generates association rules based on the co-occurrence of items in transactions. These rules reveal the associations between products and help businesses understand customer preferences, optimize product placement, and develop targeted marketing strategies."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b8676ac-c50c-48d2-94da-0942786bb672",
   "metadata": {},
   "source": [
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "Ans:\n",
    "Apriori algorithm example:\n",
    "\n",
    "Given a transaction dataset:\n",
    "\n",
    "TID Items\n",
    "1 Bread, Milk\n",
    "2 Bread, Diapers, Beer\n",
    "3 Milk, Diapers, Eggs\n",
    "4 Bread, Milk, Diapers, Beer\n",
    "5 Bread, Milk, Diapers, Eggs\n",
    "\n",
    "Find frequent 1-itemsets:\n",
    "{Bread}, {Milk}, {Diapers}\n",
    "\n",
    "Generate candidate 2-itemsets:\n",
    "{Bread, Milk}, {Bread, Diapers}, {Milk, Diapers}\n",
    "\n",
    "Find frequent 2-itemsets:\n",
    "{Bread, Milk}, {Bread, Diapers}, {Milk, Diapers}\n",
    "\n",
    "Generate candidate 3-itemsets:\n",
    "{Bread, Milk, Diapers}\n",
    "\n",
    "Find frequent 3-itemsets:\n",
    "{Bread, Milk, Diapers}\n",
    "\n",
    "Generate association rules:\n",
    "{Bread, Milk} => {Diapers}\n",
    "\n",
    "These association rules indicate that if a customer buys bread and milk, they are likely to buy diapers as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cb2ac83-7209-48fe-a800-e0c83dcc4475",
   "metadata": {},
   "source": [
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration.\n",
    "\n",
    "Ans:\n",
    "In hierarchical clustering, the distance between clusters is typically measured using a distance metric such as Euclidean distance, Manhattan distance, or correlation distance. The choice of distance metric depends on the nature of the data and the specific requirements of the clustering task.\n",
    "\n",
    "The iteration in hierarchical clustering continues until all data points are assigned to a single cluster or until a stopping criterion is met. One common stopping criterion is based on a threshold distance or a predetermined number of desired clusters. The distance metric is used to decide when to end the iteration by comparing the distances between clusters at each step.\n",
    "\n",
    "At each iteration, the algorithm merges the two closest clusters based on the chosen distance metric. The distance between clusters can be computed using different methods such as single linkage, complete linkage, or average linkage. The specific linkage method determines how the distance between clusters is calculated.\n",
    "\n",
    "The iteration ends when the distance between the remaining clusters exceeds the defined threshold or when the desired number of clusters is reached. This process forms a hierarchical structure or a dendrogram, where the vertical axis represents the distance between clusters. By setting a threshold or desired number of clusters, the algorithm determines the appropriate stopping point and partitions the data accordingly."
   ]
  },
  {
   "cell_type": "raw",
   "id": "415d1ee4-9362-4623-9ef6-421071e417fc",
   "metadata": {},
   "source": [
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "Ans:\n",
    "In the k-means algorithm, the recomputation of cluster centroids occurs in the following steps:\n",
    "\n",
    "Initialization: Initially, k centroids are randomly chosen as the center points of the clusters.\n",
    "\n",
    "Assignment: Each data point is assigned to the nearest centroid based on a chosen distance metric (usually Euclidean distance).\n",
    "\n",
    "Update: After the assignment step, the cluster memberships are fixed. The centroids are updated by computing the mean of the data points within each cluster."
   ]
  },
  {
   "cell_type": "raw",
   "id": "56e07d4f-e87f-4e67-853f-4d9157dd8367",
   "metadata": {},
   "source": [
    "6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters.\n",
    "\n",
    "Ans:\n",
    "\n",
    "The elbow method is a popular technique for determining the required number of clusters in a clustering exercise. It involves calculating the within-cluster sum of squares (WCSS) for different values of k and selecting the point on the plot where the decrease in WCSS levels off significantly as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd5b359f-430d-4ef6-9751-d3b05933c2f2",
   "metadata": {},
   "source": [
    "7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Advantages of the k-means algorithm:\n",
    "\n",
    "Simple and easy to understand.\n",
    "Computationally efficient and can handle large datasets.\n",
    "Works well with spherical and well-separated clusters.\n",
    "Converges to a local optimum, which can be sufficient for many practical applications.\n",
    "Disadvantages of the k-means algorithm:\n",
    "\n",
    "Requires the number of clusters (k) to be specified in advance.\n",
    "Sensitive to initial centroid selection, which can lead to different results.\n",
    "Assumes clusters of similar size and density.\n",
    "Not suitable for non-linear or irregularly shaped clusters.\n",
    "Can be affected by outliers, as they can disproportionately influence cluster centroids.\n",
    "Overall, while the k-means algorithm has its limitations, it remains a popular and widely used clustering algorithm due to its simplicity and efficiency."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3ff34b3-3f39-4566-b0a4-fcb6265d6207",
   "metadata": {},
   "source": [
    "8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "Ans:\n",
    "A clustering diagram consists of scattered data points in a two-dimensional space, with each point representing a data sample. Clusters are formed by grouping together data points that are close to each other, and each cluster is represented by a distinct shape or boundary. The goal is to identify natural groupings or patterns in the data based on proximity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a591bf7c-63a9-4127-9aaf-caeb1b1b67a6",
   "metadata": {},
   "source": [
    "9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "Ans:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07e3ed9c-4e34-4823-9fbd-3af04ac73b86",
   "metadata": {},
   "source": [
    "10. In a software project, the team is attempting to determine if software flaws discovered during\n",
    "testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "k-means algorithm.\n",
    "\n",
    "Ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
