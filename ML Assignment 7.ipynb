{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c93394a-f55a-4171-a461-d5f43332a24a",
   "metadata": {},
   "source": [
    "Machine Learning Assignment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eea8616-4a01-4e29-aa16-7bb4ce79cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the definition of a target function? In the sense of a real-life example, express the target\n",
    "# function. How is a target function&#39;s fitness assessed?\n",
    "\n",
    "# Ans: In machine learning, a target function is the mathematical representation of the true relationship between input variables and output variables.\n",
    "# It's the function that we are trying to approximate with a machine learning model.\n",
    "\n",
    "# A real-life example of a target function could be predicting housing prices based on various features like location, size, number of rooms, etc. \n",
    "# The target function in this case would be a mathematical function that takes in these features as input and outputs the predicted price of thehouse.\n",
    "\n",
    "# The fitness of a target function is assessed by measuring its accuracy in predicting the output variable for a given set of input variables. \n",
    "# The accuracy can be evaluated using various metrics such as mean squared error, mean absolute error, R-squared, etc.\n",
    "# A good target function should have a low error or high accuracy in predicting the output variable. The fitness of a target function is critical\n",
    "# as it determines the performance of the machine learning model built to approximate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9d441b-bbe3-4c0f-a9cc-5a7c227b28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models \n",
    "# should be provided. Distinguish between these two forms of models.\n",
    "\n",
    "# Ans: Predictive models are used to make predictions, while descriptive models are used to describe patterns and relationships in a dataset.\n",
    "# Predictive models learn the relationship between input and output variables and make predictions, while descriptive models provide insights into \n",
    "# the data without making predictions. Examples of predictive models include linear regression and neural networks, while examples of descriptive\n",
    "# models include clustering and association rule mining.\n",
    "# A real life example would be in a sales prediction kind of task, the predictive model will predict when will a particular customer is likely to \n",
    "# make a puchase, based on several features such as past purchase history, day, week, month etc. A descriptive model can be used for identify \n",
    "# some behaviour of the customer, what kind of category do they fall, what kind of customers are going to mkae large purchases etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b6ce9d-5d11-4ff6-b2aa-503dd5d102a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various\n",
    "# measurement parameters.\n",
    "\n",
    "# Ans: To assess the efficiency of a classification model, we need to evaluate its performance on a test dataset.\n",
    "# The most commonly used evaluation metrics for classification models include accuracy, precision, recall, F1 score, and area under the ROC curve \n",
    "# (AUC-ROC). Let's discuss each of these metrics in more detail:\n",
    "\n",
    "# Accuracy: This is the most straightforward metric and measures the proportion of correct predictions out of total predictions. \n",
    "# It's calculated by dividing the number of correct predictions by the total number of predictions. However, accuracy can be misleading\n",
    "# if the classes are imbalanced.\n",
    "\n",
    "# Precision: Precision measures the proportion of true positives among all predicted positives. It's calculated by dividing the number\n",
    "# of true positives by the sum of true positives and false positives. High precision means that the model is making few false positive errors.\n",
    "\n",
    "# Recall: Recall measures the proportion of true positives among all actual positives. It's calculated by dividing the number of true \n",
    "# positives by the sum of true positives and false negatives. High recall means that the model is identifying most of the actual positives.\n",
    "\n",
    "# F1 score: F1 score is the harmonic mean of precision and recall and provides a balance between the two metrics. It's calculated as\n",
    "# 2*(precision*recall)/(precision+recall). F1 score is a good metric to use when we want to consider both precision and recall.\n",
    "\n",
    "# AUC-ROC: AUC-ROC measures the model's ability to distinguish between positive and negative classes across different threshold values.\n",
    "# It plots the true positive rate (recall) against the false positive rate (1-specificity) at various threshold values. AUC-ROC provides a\n",
    "# single value that summarizes the model's overall performance.\n",
    "\n",
    "# In summary, evaluating a classification model's performance involves assessing its accuracy, precision, recall, F1 score, and AUC-ROC.\n",
    "# Depending on the problem and the class distribution, some of these metrics may be more important than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deedc846-9f18-45fe-9500-f942293cd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "# ii. What does it mean to overfit? When is it going to happen?\n",
    "# iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
    "\n",
    "# Ans: i. Underfitting occurs when a machine learning model is not able to capture the underlying patterns in the data, resulting in poor performance\n",
    "# on both the training and testing datasets. The most common reason for underfitting is using a model that is too simple or has too few parameters\n",
    "# relative to the complexity of the data.\n",
    "\n",
    "# ii. Overfitting occurs when a machine learning model fits the training data too closely and captures noise or random fluctuations, resulting in \n",
    "# poor generalization to new data. Overfitting often happens when the model is too complex relative to the amount of available training data.\n",
    "\n",
    "# iii. The bias-variance trade-off is a key concept in model fitting that describes the trade-off between model complexity (variance) and model \n",
    "# accuracy (bias). A model with high bias (e.g., a simple linear model) is often too rigid and unable to capture the complexity of the data, \n",
    "# resulting in underfitting. A model with high variance (e.g., a complex neural network) is often too flexible and captures noise or random \n",
    "# fluctuations, resulting in overfitting. The goal of model selection is to find a balance between bias and variance that results in the best \n",
    "# overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92262aae-0671-44c0-a9f5-fc4000a7b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
    "\n",
    "# Ans: Increasing the size of the training dataset: A larger dataset can help the model learn more complex patterns and reduce overfitting.\n",
    "\n",
    "# Improving the quality of the input data: Better quality data can improve the performance of the model.\n",
    "\n",
    "# Tuning hyperparameters: Hyperparameters such as learning rate, regularization strength, and number of hidden layers can significantly impact the\n",
    "# performance of the model.\n",
    "\n",
    "# Using more complex models: More complex models such as deep neural networks can improve performance on complex tasks, \n",
    "# but they can also be prone to overfitting.\n",
    "\n",
    "# Using ensembles: Combining the predictions of multiple models (e.g., random forests or gradient boosting) can improve performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114baa56-0ccd-4fe8-8d28-14566289343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. How would you rate an unsupervised learning model&#39;s success? What are the most common success indicators for an unsupervised learning model?\n",
    "\n",
    "# Ans: In unsupervised learning, there is no explicit target variable, and the goal is to find patterns or structure in the data. Therefore,\n",
    "# success is often evaluated based on the model's ability to uncover meaningful patterns or groupings in the data that can be useful for \n",
    "# downstream tasks such as visualization, clustering, or anomaly detection.\n",
    "\n",
    "# The most common success indicators for an unsupervised learning model include:\n",
    "\n",
    "# Clustering performance: If the goal of the model is to cluster data points into meaningful groups, clustering performance metrics such as silhouette\n",
    "# score or homogeneity score can be used to evaluate the quality of the clusters.\n",
    "\n",
    "# Visualization quality: If the goal of the model is to visualize high-dimensional data in a lower-dimensional space, the quality of the resulting \n",
    "# visualizations can be evaluated by assessing how well they capture the underlying structure of the data.\n",
    "\n",
    "# Anomaly detection performance: If the goal of the model is to detect anomalies or outliers in the data, the success of the model can be \n",
    "# evaluated based on how well it identifies known anomalies or how well it generalizes to new data.\n",
    "\n",
    "# Overall, the success of an unsupervised learning model depends on the specific task and the intended use case, so it's important to carefully\n",
    "# evaluate the model based on relevant metrics and consider the limitations of the model in the context of the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9db749-58fd-415b-835c-a4767fbb356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? \n",
    "# Explain your answer.\n",
    "\n",
    "# Ans: No, it is not appropriate to use a classification model for numerical data or a regression model for categorical data.\n",
    "\n",
    "# Classification models are designed to predict discrete categorical values (e.g., yes or no, red or blue), while regression models are\n",
    "# designed to predict continuous numerical values (e.g., temperature, stock prices). If we try to use a classification model for numerical data,\n",
    "# the model will not be able to make meaningful predictions since the output will be limited to a discrete set of categories. Similarly, \n",
    "# if we use a regression model for categorical data, the model may produce predictions that are outside of the expected range of values,\n",
    "# which can be misleading and result in poor performance.\n",
    "\n",
    "# Therefore, it is important to select the appropriate type of model based on the nature of the data and the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4986c14-754b-4e0b-928a-754c27dd1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "\n",
    "# Ans: Predictive modeling for numerical values involves using regression models to predict continuous values. \n",
    "# These models learn a function that maps input features to a numerical output, and they are commonly evaluated using metrics such as mean \n",
    "# squared error or R-squared.\n",
    "# \n",
    "# In contrast, categorical predictive modeling involves using classification models to predict discrete categorical values. These models learn a \n",
    "# decision boundary that separates different classes of data, and they are commonly evaluated using metrics such as accuracy or F1 score.\n",
    "\n",
    "# The main difference between these two types of modeling is the nature of the output variable. While numerical modeling predicts continuous values, \n",
    "# categorical modeling predicts discrete categories. Therefore, the appropriate modeling approach depends on the nature of the data and the \n",
    "# specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbf1911-7472-4685-b9cf-5eb0cebc00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. The following data were collected when using a classification model to predict the malignancy of a group of patients&#39; tumors:\n",
    "# i. Accurate estimates – 15 cancerous, 75 benign\n",
    "# ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "# The error rate is the proportion of incorrect predictions made by the model, which is (3 + 7) / 100 = 0.1 or 10%.\n",
    "\n",
    "# To calculate the Kappa value, we need to know the prevalence of each class in the dataset. Assuming an equal prevalence of cancerous and benign \n",
    "# tumors, the expected accuracy due to chance is (15 + 75) / 100 = 0.9 or 90%. The Kappa value is then \n",
    "# (accuracy - expected accuracy) / (1 - expected accuracy) = (0.9 - 0.84) / (1 - 0.84) = 0.33.\n",
    "\n",
    "# The sensitivity is the proportion of actual cancerous tumors that were correctly identified by the model, which is 15 / (15 + 3) = 0.83 or 83%.\n",
    "\n",
    "# The precision is the proportion of tumors predicted as cancerous that are actually cancerous, which is 15 / (15 + 7) = 0.68 or 68%.\n",
    "\n",
    "# The F-measure is the harmonic mean of precision and recall, and provides a balanced measure of the model's performance.\n",
    "# The F-measure is 2 * precision * recall / (precision + recall) = 2 * 0.68 * 0.83 / (0.68 + 0.83) = 0.75 or 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512415c4-8f6b-4ed7-a987-015f362c1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Make quick notes on:\n",
    "# 1. The process of holding out\n",
    "# 2. Cross-validation by tenfold\n",
    "# 3. Adjusting the parameters\n",
    "\n",
    "# Ams: The process of holding out involves splitting the available data into two separate sets: a training set and a validation set. \n",
    "# The training set is used to fit the model, while the validation set is used to assess the model's performance on unseen data.\n",
    "\n",
    "# Cross-validation by tenfold is a technique for evaluating the performance of a model by dividing the data into ten equally sized subsets \n",
    "# or \"folds.\" The model is trained on nine of the folds and tested on the remaining fold. This process is repeated ten times, with each fold\n",
    "# serving as the test set once. The results are then averaged to obtain a more robust estimate of the model's performance.\n",
    "\n",
    "# Adjusting the parameters involves changing the values of the model's hyperparameters to improve its performance on the validation set. \n",
    "# Hyperparameters are set before training the model and control aspects such as the learning rate, regularization strength, and model \n",
    "# complexity. A common approach is to perform a grid search over a range of hyperparameter values and select the combination that yields the best \n",
    "# performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5f091b-3432-466f-8fe1-b91fa0f23eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define the following terms:\n",
    "# 1. Purity vs. Silhouette width\n",
    "# 2. Boosting vs. Bagging\n",
    "# 3. The eager learner vs. the lazy learner\n",
    "\n",
    "# Ans: Purity is a measure of the homogeneity of clusters in a clustering algorithm, with higher purity indicating that each cluster\n",
    "# contains a high proportion of a single class. Silhouette width is a measure of the quality of the clustering, taking into account\n",
    "# both the distance between data points within a cluster and the distance between data points in different clusters.\n",
    "\n",
    "# Boosting and bagging are both ensemble learning techniques used to improve the performance of machine learning models. \n",
    "# Boosting involves training multiple weak models sequentially, with each new model attempting to correct the errors of the previous model. \n",
    "# Bagging involves training multiple models in parallel on randomly sampled subsets of the training data, with each model contributing equally to \n",
    "# the final prediction.\n",
    "\n",
    "# The eager learner and the lazy learner are two types of machine learning algorithms. Eager learners build a model based on the entire training set \n",
    "# before making predictions on new data, while lazy learners delay building the model until new data is presented for prediction. Eager learners \n",
    "# include models such as decision trees and neural networks, while lazy learners include models such as k-nearest neighbors and locally weighted \n",
    "# regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c6055-2424-4574-a5f2-97c49bb0b8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
